{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO5Bt7mLvIIw0mcaPig8izT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Q-Learning"],"metadata":{"id":"qBA4hcOVItUK"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Define the environment (grid world)\n","# 'S' denotes the start position\n","# 'G' denotes the goal position\n","# 'H' denotes a hazard (negative reward)\n","# '.' denotes a normal cell (no reward)\n","# The agent can move in four directions: up (0), down (1), left (2), right (3)\n","\n","environment = np.array([\n","    ['S', '.', '.', '.'],\n","    ['.', 'H', '.', 'H'],\n","    ['.', '.', '.', 'H'],\n","    ['H', '.', '.', 'G']\n","])\n","\n","# Define the rewards for each cell\n","# 'G' (goal) has a high positive reward\n","# 'H' (hazard) has a high negative reward\n","# '.' (normal cell) has a neutral reward\n","\n","reward_map = {\n","    'S': 0,  # start\n","    'G': 10,  # goal\n","    'H': -10,  # hazard\n","    '.': 0  # normal cell\n","}\n","\n","# Define Q-learning parameters\n","num_episodes = 1000\n","learning_rate = 0.1\n","discount_factor = 0.9\n","epsilon = 0.1  # exploration-exploitation tradeoff parameter\n","\n","# Initialize Q-table\n","num_states = np.prod(environment.shape)\n","num_actions = 4  # up, down, left, right\n","Q = np.zeros((num_states, num_actions))\n","\n","# Helper function to convert state coordinates to index\n","def state_to_index(state):\n","    return state[0] * environment.shape[1] + state[1]\n","\n","# Helper function to choose an action using epsilon-greedy policy\n","def choose_action(state):\n","    if np.random.rand() < epsilon:\n","        return np.random.randint(num_actions)  # explore\n","    else:\n","        return np.argmax(Q[state_to_index(state), :])  # exploit\n","\n","# Q-learning algorithm\n","for episode in range(num_episodes):\n","    state = (0, 0)  # start at the top-left corner\n","    total_reward = 0\n","\n","    while True:\n","        action = choose_action(state)\n","        next_state = None\n","\n","        # Determine next state based on action\n","        if action == 0:  # up\n","            next_state = (max(state[0] - 1, 0), state[1])\n","        elif action == 1:  # down\n","            next_state = (min(state[0] + 1, environment.shape[0] - 1), state[1])\n","        elif action == 2:  # left\n","            next_state = (state[0], max(state[1] - 1, 0))\n","        elif action == 3:  # right\n","            next_state = (state[0], min(state[1] + 1, environment.shape[1] - 1))\n","\n","        # Get reward for the next state\n","        reward = reward_map[environment[next_state]]\n","\n","        # Update Q-value using Bellman equation\n","        current_index = state_to_index(state)\n","        next_index = state_to_index(next_state)\n","        Q[current_index, action] = (1 - learning_rate) * Q[current_index, action] \\\n","                                   + learning_rate * (reward + discount_factor * np.max(Q[next_index, :]))\n","\n","        # Move to the next state\n","        state = next_state\n","        total_reward += reward\n","\n","        # Break if goal is reached\n","        if environment[next_state] == 'G':\n","            break\n","\n","    # Print episode information\n","    if (episode + 1) % 100 == 0:\n","        print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}\")\n","\n","# Test the learned policy\n","state = (0, 0)\n","path = [state]\n","\n","while True:\n","    action = np.argmax(Q[state_to_index(state), :])\n","    next_state = None\n","\n","    if action == 0:\n","        next_state = (max(state[0] - 1, 0), state[1])\n","    elif action == 1:\n","        next_state = (min(state[0] + 1, environment.shape[0] - 1), state[1])\n","    elif action == 2:\n","        next_state = (state[0], max(state[1] - 1, 0))\n","    elif action == 3:\n","        next_state = (state[0], min(state[1] + 1, environment.shape[1] - 1))\n","\n","    path.append(next_state)\n","    state = next_state\n","\n","    if environment[next_state] == 'G':\n","        break\n","\n","# Print the path taken by the agent\n","print(\"Path taken by the agent:\")\n","for row in environment:\n","    print(' '.join(row))\n","print(\"Agent's path:\")\n","for step in path:\n","    environment[step] = '*'\n","    for row in environment:\n","        print(' '.join(row))\n","    environment[step] = '.'\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yzUh5-8DHhwy","executionInfo":{"status":"ok","timestamp":1713073202817,"user_tz":-480,"elapsed":30008,"user":{"displayName":"Hui Wen Nies","userId":"05887277120475105053"}},"outputId":"91009478-5144-44b9-dedd-1e5ac4de40d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 100/1000, Total Reward: -720\n","Episode 200/1000, Total Reward: -270\n","Episode 300/1000, Total Reward: 10\n","Episode 400/1000, Total Reward: 10\n","Episode 500/1000, Total Reward: 0\n","Episode 600/1000, Total Reward: 10\n","Episode 700/1000, Total Reward: 10\n","Episode 800/1000, Total Reward: 10\n","Episode 900/1000, Total Reward: 10\n","Episode 1000/1000, Total Reward: 10\n","Path taken by the agent:\n","S . . .\n",". H . H\n",". . . H\n","H . . G\n","Agent's path:\n","* . . .\n",". H . H\n",". . . H\n","H . . G\n",". * . .\n",". H . H\n",". . . H\n","H . . G\n",". . * .\n",". H . H\n",". . . H\n","H . . G\n",". . . .\n",". H * H\n",". . . H\n","H . . G\n",". . . .\n",". H . H\n",". . * H\n","H . . G\n",". . . .\n",". H . H\n",". . . H\n","H . * G\n",". . . .\n",". H . H\n",". . . H\n","H . . *\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize the features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Q-learning parameters\n","num_states = X_train_scaled.shape[0]  # Number of training instances\n","num_actions = len(np.unique(y_train))  # Number of unique classes in y_train\n","\n","# Initialize Q-table\n","Q = np.zeros((num_states, num_actions))\n","\n","# Q-learning hyperparameters\n","alpha = 0.5  # Learning rate\n","gamma = 0.9  # Discount factor\n","epsilon = 1.0  # Exploration rate\n","epsilon_decay = 0.995  # Exploration decay rate\n","num_episodes = 1000\n","\n","# Q-learning algorithm\n","for episode in range(num_episodes):\n","    state = np.random.randint(0, num_states)  # Start from a random state\n","\n","    while True:\n","        # Choose action (class prediction) using epsilon-greedy policy\n","        if np.random.rand() < epsilon:\n","            action = np.random.randint(0, num_actions)  # Explore\n","        else:\n","            action = np.argmax(Q[state])  # Exploit\n","\n","        # Simulate taking the action and observe the next state and reward\n","        next_state = state\n","        reward = 1 if y_train[state] == action else -1\n","\n","        # Update Q-value using the Bellman equation\n","        Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n","\n","        state = next_state  # Move to the next state\n","\n","        # Decay epsilon to reduce exploration over time\n","        epsilon *= epsilon_decay\n","\n","        if np.random.rand() < 0.1:\n","            break  # Break episode with a small probability to explore new episodes\n","\n","# Function to predict classes using the learned Q-table\n","def predict_classes(X, Q):\n","    y_pred = []\n","    for i in range(X.shape[0]):\n","        state = i % num_states  # Use modulo to handle cases where X is larger than Q-table\n","        action = np.argmax(Q[state])  # Choose the action with the highest Q-value\n","        y_pred.append(action)\n","    return np.array(y_pred)\n","\n","# Make predictions on the test set using the learned Q-table\n","y_pred = predict_classes(X_test_scaled, Q)\n","\n","# Evaluate accuracy of the predictions\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy using Q-learning:\", accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oBPRagMk2tVh","executionInfo":{"status":"ok","timestamp":1713086853427,"user_tz":-480,"elapsed":975,"user":{"displayName":"Hui Wen Nies","userId":"05887277120475105053"}},"outputId":"10abe661-8cea-4373-bd9c-4c8ec7d75a34"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy using Q-learning: 0.4\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize the features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Initialize and train a Support Vector Machine (SVM) classifier\n","svm = SVC(kernel='rbf', random_state=42)\n","svm.fit(X_train_scaled, y_train)\n","\n","# Make predictions on the test set\n","y_pred = svm.predict(X_test_scaled)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy:\", accuracy)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IWfVDP078OBg","executionInfo":{"status":"ok","timestamp":1713087150563,"user_tz":-480,"elapsed":1013,"user":{"displayName":"Hui Wen Nies","userId":"05887277120475105053"}},"outputId":"d369f15b-feff-4682-db41-6467850083bb"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 1.0\n"]}]}]}